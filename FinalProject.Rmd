---
title: "Practical Machine Learning Final Project"
author: "Disha An"
date: "10/15/2017"
output: html_document
---
#### Goals: 

* Use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

* Predict the manner in which they did the exercise. The "classe" variable is the manner.

* Describe how did I build the model.

* How did I use cross validation.

* What do I think the expected out of sample error is.

* Why I made my choices.

#### Data:

- [training data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
- [testing data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

#### Get Data, Data Cleaning & Data Summary
```{r}
library(data.table)
train<- fread('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
dim(train)
```
The training data has 19622 rows and 160 columns.
There too many variables for predicting as well as a lot of NAs. So I decided to delete the variables which NAs are more than 10%.
```{r}
head(train)
```

```{r}
train_df <- as.data.frame(train)
train_df <- train_df[, -(which(colMeans(is.na(train_df)) > 0.10))]
for (i in 1:ncol(train_df)) {
        for (j in 1:nrow(train_df)) {
                if (train_df[j, i] == ""){
                        train_df[j,i] = NA
                }
        }
}
train_df <- train_df[, -(which(colMeans(is.na(train_df)) > 0.10))]
```

```{r}
dim(train_df)
```
After getting rid of the the NAs variables, we still have 60 variables.

```{r}
train_df = subset(train_df,select = -c(V1))
head(train_df)
```
Classes are: A B C D E
Now we use the nearZeroVar function to get rid of the variables with near zero variance property.
```{r}
library(caret)
nzv <- nearZeroVar(train_df)
filtered_train <- train_df[,-nzv]
dim(filtered_train)
```
We have 58 variables now.

#### Prediction with Trees
Using the Machine Learning Algorithm to predict.
Firstly, I want to split the training data to newtraining and validation
```{r}
library(caret)
set.seed(1234)
inTrain <- createDataPartition(y = filtered_train$classe,
                              p = 0.7, list = FALSE)
training <- filtered_train[inTrain,]
validation <- filtered_train[-inTrain,]
head(training)
```
```{r}
head(validation)
```
```{r}
set.seed(6666)
library(rpart)
mod1 <- rpart(classe ~., data = training, method = "class")
```

Use this model 1 to predict the validation dataset
```{r}
pred1 <- predict(mod1, validation, type = "class")
table(pred1, validation$classe)
dim(validation)
```

So accuracy for this method is
```{r}
(1606+947+930+752+900)/5885
```

#### Prediction with Random Forest

```{r}
set.seed(6666)
mod2 <- train(classe~., data = training, method = "rf", 
               trControl = trainControl(method="cv", 5),
               importance = TRUE, ntree = 100)
```
User this model 2 to predict our validation dataset

```{r}
pred2 <- predict(mod2, validation)
table(pred2, validation$classe)
dim(validation)
```
So the accuracy for this model is:
```{r}
(1674+1138+1025+963+1081)/5885
```

#### Results Comparison
The accuracy for validation dataset in decision tree is less than the one in random forest. So I decide to use random forest tree for our final prediction of the original testing dataset.

We need to get rid off the variables which should be deleted in the testing dataset.
```{r}
test<- fread('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')
dim(test)
```

```{r}
train_name <- names(training)
train_name
```

```{r}
library(dplyr)
temp <- as.character(train_name)
test <- test[,(names(test) %in% temp)]
dim(test)
```
Now, we use the random forest model to predict the test data.
Train dataset will be the filtered train. 

```{r}
set.seed(6666)
library(caret)
modle_final <- train(classe~., data = filtered_train, method = "rf", 
               trControl = trainControl(method="cv", 5),
               importance = TRUE, ntree = 100)
```
Get the predicted result.
```{r}
prediction <- predict(modle_final, test)
prediction
```